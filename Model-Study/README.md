# Model-Study Folder

Here it is available the Jupyter Notebooks created to create the classification models.

## Tools
> Python <br>
> Scikit-learn

## Bibliography

1.	LEITE CRISTOFARO, R. A. et al. Artificial Intelligence Strategy Minimizes Lost Circulation Non-Productive Time in Brazilian Deep Water Pre-Salt. OTC Brasil. [S.l.]: [s.n.]. 2017.
2.	TOREIFI, H.; ROSTAMI, H. New method for prediction and solving the problem of drilling fluid loss using modular neural network and particle swarm optimization algorithm. Journal of Petroleum Exploration and Production Technology, v. 4, p. 371-379, 2014.
3.	FERNANDES, A. A. Modelagem e avaliação de controladores não lineares para perfuração de poços com gerenciamento de pressão. Pontifícia Universidade Católica do Rio de Janeiro. [S.l.]. 2015.
4.	OLIVEIRA, R. G. Otimização de Estratégias de Controle, Localização e Quantidade de Válvulas de Poços Inteligentes com Computação Evolucionária de Variáveis Híbridas. CENTRO FEDERAL DE EDUCAÇÃO TECN. CELSO SUCKOW DA FONSECA. [S.l.]. 2017.
5.	CAMPOS, O.; OTHERS. Saving Capex on Further Development of Complex Mature Fields with MPD in Deep Water Drilling-Roncador Case. OTC Brasil. [S.l.]: [s.n.]. 2017.
6.	THOMAS, J. E. Fundamentos de Engenharia de Petróleo. 2ª edição. [S.l.]: [s.n.], 2004.
7.	FERNANDES, A. A. et al. Overview of Different Applications for MPD and MCD Techniques on Deepwater Wells. IADC/SPE Managed Pressure Drilling & Underbalanced Operations Conference & Exhibition. [S.l.]: [s.n.]. 2017.
8.	INTERNATIONAL Association of Drilling Contractors. Página do IADC, 2019, tradução nossa. Disponivel em: <http://www.iadclexicon.org/mpd/>. Acesso em: 5 março 2019.
9.	BACON, W. et al. MPD dynamic influx control mitigates conventional well control pitfalls. SPE/IADC Managed Pressure Drilling and Underbalanced Operations Conference and Exhibition. [S.l.]: [s.n.]. 2016.
10.	GÉRON, A. Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. [S.l.]: "O'Reilly Media, Inc.", 2017.
11.	TAN, P.-N.; STEINBACH, M.; KUMAR, V. Introduction to data mining. [S.l.]: Pearson Education India, 2006.
12.	THEODORIDIS, S.; KOUTROUMBAS, K. Pattern Recognition. [S.l.]: Elsevier, 2008.
13.	WITTEN, I. H. et al. Data Mining: Practical machine learning tools and techniques. [S.l.]: Morgan Kaufmann, 2016.
14.	LLOYD, S. Least squares quantization in PCM. IEEE transactions on information theory, v. 28, p. 129-137, 1982.
15.	PEDREGOSA, F. et al. Scikit-learn: Machine Learning in Python. Journal of Machine Learning Research, v. 12, p. 2825-2830, 2011.
16.	FACELLI, K. et al. Inteligência Artificial: Uma Abordagem de Aprendizado de Maquina. 1°. ed. Rio de Janeiro: LTC, 2011.
17.	OLIVEIRA, T. C. D. C. Curvas principais hierárquicas para a classificação de sinais de sonar passivo. CENTRO FEDERAL DE EDUCAÇÃO TECN. CELSO SUCKOW DA FONSECA. [S.l.]. 2017.
18.	HASTIE, T.; TIBSHIRANI, R.; FRIEDMAN, J. The Elements of Statistical Learning. second edition. ed. Stanford: Springer, 2008. ISBN 9780387848587.
19.	CHEN, R.; HERSKOVITS, E. H. Machine-learning techniques for building a diagnostic model for very mild dementia. Neuroimage, v. 52, p. 234-244, 2010.
20.	KOHAVI, R. A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection. International Joint Conference on Articial Intelligence. Stanford: [s.n.]. 1995. p. 1137–1145.
21.	ARLOT, S.; CELISSE, A. A Survey of Cross-Validation Procedures for Model Selection. Statistics Surveys, Paris, vol.4, DOI:10.1214/09-SS054, 2010. p.40-79.
22.	REFAEILZADEH, P.; TANG, L.; LIU, H. Cross-Validation. In: LIU, L.; OZSU, M. T. Encyclopedia of Databa Systems. Tempe: Springer, 2009. p. 532-538. ISBN 978-0-387-35544-3.
23.	REZENDE, S. O. Sistemas Inteligentes - Fundamentos e Aplicações. 1ª. ed. Barueri: Manole, 2005. p. 109-111. ISBN 85-204-1683-7.
24.	TAN, P. N.; STEINBACK, M.; KUMAR, V. Introduction to Data Mining. 1ª. ed. Edimburgh: Pearson, 2014. ISBN 0321321367.
25.	EFRON, B. Estimating the Error Rate of a Prediction Rule: Improvement on Cross Validation. Jornal of American Statisitcal Association, vol. 78, no.382, 1983. p. 316-331.
26.	FAWCETT, T. An introduction to ROC analysis. Pattern recognition letters, v. 27, p. 861-874, 2006.
27.	EGAN, J. P. Signal Detection Theory and ROC Analysis. New York: Academic Press, 1975. ISBN 0122328503.
28.	TILAKI, K. Receiver Operating Characteristic (ROC) Curve Analysis for Medical Diagnostic Test Evaluation. Caspian Journal of Internal Medicine, 2013. p. 627–635.
29.	ZOU, K. H.; O’MALLEY, A. J.; MAURI, L. Receiver-operating characteristic analysis for evaluating diagnostic tests and predictive models. Circulation, v. 115, p. 654-657, 2007.
30.	SPACKMAN,. Signal Detection Theory: Valuable Tools for Evaluating Inductive Learning. Morgan Kaufman. Sixth International Workshop on Machine Learning. San Mateo: [s.n.]. 1989. p. 160–163.
31.	LING, C. X.; HUANG, J.; ZHANG, H. AUC: A Statistically Consistent and More Discriminating Measure than Accuracy. In 18th international joint conference on Artificial intelligence (IJCAI'03). San Francisco: Morgan Kaufmann Publishers. 2003.
32.	LECUN, Y.; BENGIO, Y.; HINTON, G. Deep learning. Nature, v. 521, p. 436, 2015.
33.	WOLPERT, D. H. The lack of a priori distinctions between learning algorithms. Neural computation, v. 8, p. 1341-1390, 1996.
34.	FILMER, D.; PRITCHETT, L. Estimating wealth effects without expenditure data—or tears. Policy Research Working Paper 1980, The World. [S.l.]: [s.n.]. 1998.
35.	FILMER, D.; PRITCHETT, L. H. Estimating wealth effects without expenditure data—or tears: an application to educational enrollments in states of India. Demography, v. 38, p. 115-132, 2001.
36.	KOLENIKOV, S.; ANGELES, G. The use of discrete data in PCA: theory, simulations, and applications to socioeconomic indices. Chapel Hill: Carolina Population Center, University of North Carolina, p. 1-59, 2004.
37.	NIITSUMA, H.; OKADA, T. Covariance and PCA for categorical variables. Pacific-Asia Conference on Knowledge Discovery and Data Mining. [S.l.]: [s.n.]. 2005. p. 523-528.
38.	BUNTINE, W.; JAKULIN, A. Applying discrete PCA in data analysis. Proceedings of the 20th conference on Uncertainty in artificial intelligence. [S.l.]: [s.n.]. 2004. p. 59-66.
39.	GINSBURG, S. et al. Variable ranking with pca: Finding multiparametric mr imaging markers for prostate cancer diagnosis and grading. International Workshop on Prostate Cancer Imaging. [S.l.]: [s.n.]. 2011. p. 146-157.
40.	HUA, J. et al. Optimal number of features as a function of sample size for various classification rules. Bioinformatics, v. 21, p. 1509-1515, 2004.
41.	HASTIE, T.; ROBERT, T.; FRIEDMAN, J. H. The elements of statistical learning: data mining, inference, and prediction. [S.l.]: New York, NY: Springer, 2009.
42.	GUYON, I.; ELISSEEFF, A. An introduction to variable and feature selection. Journal of machine learning research, v. 3, p. 1157-1182, 2003.
43.	KITTLER, J.; HATER, M.; DUIN, R. P. W. Combining classifiers. Proceedings of 13th international conference on pattern recognition. [S.l.]: [s.n.]. 1996. p. 897-901.
44.	RUMELHART, D. E.; HINTON, G. E.; WILLIAMS, R. J. Learning internal representations by error propagation. California Univ San Diego La Jolla Inst for Cognitive Science. [S.l.]. 1985.
45.	YOHANNES, Y.; WEBB, P. Classification and Regression Trees, CART: A User Manual For Identifying Indicators of Vulnerability to Famine Chronic Food Insecurity. Washington: International Food Policy Research Institute, 1999. ISBN 0-89629 337 8.

## Licence: MIT
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
